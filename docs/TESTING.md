# Testing Guide

This document describes the testing infrastructure and best practices for the project.

## Overview

The project uses a comprehensive testing strategy with:

- **.NET**: xUnit with coverage tracking
- **Python**: pytest for HPC runner tests
- **JavaScript/TypeScript**: Vitest for client and worker tests
- **CI/CD**: GitHub Actions for automated testing
- **Coverage**: Codecov integration for coverage reports

## Running Tests

### All Tests

```bash
# Run all tests
just test

# Run with coverage
just test-coverage
```

### .NET Tests

```bash
# Run .NET tests only
just test-dotnet

# With coverage
just test-dotnet-coverage

# Or manually
dotnet test
```

### Python Tests

```bash
# Run Python tests only (hpc_runner)
just test-python

# With coverage
just test-python-coverage

# Or manually
cd hpc_runner && uv run pytest
```

### JavaScript/TypeScript Tests

```bash
# Run JS/TS tests only
just test-js

# With coverage
just test-js-coverage

# Or manually
cd client && npm test
cd workers && npm test
```

## Test Structure

### .NET (server & tests)

```
tests/
└── server.tests/
    ├── server.tests.csproj       # xUnit project file
    ├── DomainModelTests.cs       # Domain model serialization tests
    ├── JobControllerTests.cs     # Controller tests
    └── MetricsClientTests.cs     # Metrics client tests
```

**Conventions:**

- Test files: `*Tests.cs`
- Test methods: `[Fact]` or `[Theory]` attributes
- Use `xUnit` assertions
- Use Moq for mocking

### Python (hpc_runner)

```
hpc_runner/
├── pyproject.toml             # pytest configuration
└── tests/
    ├── test_ai_processors.py  # AI processor tests
    └── test_runner.py         # Main runner tests
```

**Conventions:**

- Test files: `test_*.py`
- Test functions: `test_*`
- Use fixtures for setup/teardown
- Use `pytest.mark` for categorization

### Client (React + Vite + Vitest)

```
client/
├── src/
│   ├── lib/
│   │   ├── api.ts
│   │   └── __tests__/
│   │       └── api.test.ts
│   └── test/
│       └── setup.ts
├── vitest.config.ts
└── package.json
```

**Conventions:**

- Test files: `*.test.tsx` or `*.test.ts`
- Co-locate tests in `__tests__/` near the code they exercise
- Use Vitest (React Testing Library is available for UI tests as the frontend grows)

### Workers (Vitest)

```
workers/
├── api/
│   ├── job-status.ts
│   ├── submit-job.ts
│   └── upload.ts
├── tests/
│   ├── job-status.test.ts
│   ├── submit-job.test.ts
│   └── upload.test.ts
└── vitest.config.ts
```

**Conventions:**

- Test files: `*.test.ts`
- Tests currently focus on the `workers/api/*` prototype scripts and include stubs/TODOs
- Use Vitest's built-in mocking and keep worker behavior testable in isolation

## Writing Tests

### .NET Example

```csharp
using Xunit;

public class ModuleTests
{
    [Fact]
    public void TestFunction()
    {
        // Arrange
        var expected = "value";
        
        // Act
        var result = MyFunction();
        
        // Assert
        Assert.Equal(expected, result);
    }
}
```

### Python Example

```python
"""Tests for module."""

import pytest

def test_function():
    """Test description."""
    result = my_function()
    assert result == expected
    
@pytest.fixture
def mock_data():
    """Fixture description."""
    return {"key": "value"}
```

### TypeScript Example

```typescript
import { describe, it, expect } from 'vitest'

describe('Module', () => {
  it('should work', () => {
    const result = myFunction()
    expect(result).toBe(expected)
  })
})
```

## Coverage

Coverage is collected locally (via `just test-coverage`) and in CI, and uploaded to Codecov (best-effort).

- There is no enforced repo-wide coverage threshold today.
- Local HTML reports:
  - Python: `hpc_runner/htmlcov/`
  - Client: `client/coverage/`
  - Workers: `workers/coverage/`
  - .NET: `coverage/` (generated by `just test-dotnet-coverage`)

### View Coverage Locally

```bash
# .NET
just test-dotnet-coverage
# Output: ./coverage/ (includes Summary.txt)

# Python (hpc_runner)
just test-python-coverage
open hpc_runner/htmlcov/index.html

# Client
cd client && npm run test:coverage
open coverage/index.html

# Workers
cd workers && npm run test:coverage
open coverage/index.html
```

## CI/CD Integration

Tests run automatically on:

- Every push to `main`
- Every pull request

### GitHub Actions

- `.github/workflows/ci.yml` runs:
  - .NET lint (`dotnet build` with warnings as errors)
  - .NET tests with coverage (uploads Cobertura XML)
  - Python lint (ruff + mypy) and tests with coverage (uploads `hpc_runner/coverage.xml`)
  - Client tests with coverage (uploads `client/coverage/lcov.info`)
  - Workers lint (eslint) and tests with coverage (uploads `workers/coverage/coverage-final.json`)
- `.github/workflows/security.yml` runs `dotnet list package --vulnerable`, `npm audit` (client/workers), and `pip-audit` (hpc_runner).
- `.github/workflows/codeql.yml` runs CodeQL scanning.

### Workflow Status

View test results in the GitHub Actions tab or PR checks

## Best Practices

### Do's

✅ Write tests for new features
✅ Test edge cases and error conditions
✅ Use descriptive test names
✅ Keep tests isolated and independent
✅ Use fixtures/mocks for external dependencies
✅ Run tests locally before pushing

### Don'ts

❌ Don't test implementation details
❌ Don't skip tests without good reason
❌ Don't commit failing tests
❌ Don't write flaky tests
❌ Don't test third-party code

## Debugging Tests

### .NET

```bash
# Run specific test
dotnet test --filter "FullyQualifiedName=Namespace.ClassName.TestMethod"

# Verbose output
dotnet test -v detailed

# Debug mode
dotnet test --logger "console;verbosity=detailed"
```

### Python

```bash
# Run specific test
cd hpc_runner && uv run pytest tests/test_runner.py::test_function

# Verbose output
cd hpc_runner && uv run pytest -v

# Show print statements
cd hpc_runner && uv run pytest -s

# Stop on first failure
cd hpc_runner && uv run pytest -x

# Interactive debugging
cd hpc_runner && uv run pytest --pdb
```

### JavaScript/TypeScript

```bash
# Run a specific test file (client)
cd client && npm test -- src/lib/__tests__/api.test.ts

# Run a specific test file (workers)
cd workers && npm test -- tests/upload.test.ts

# Watch mode
cd client && npm run test:watch
cd workers && npm run test:watch
```

## Test Dependencies

### .NET Dependencies

- `Microsoft.NET.Test.Sdk` - Test runner integration
- `xunit.v3` - Test framework
- `xunit.runner.visualstudio` - Test runner adapter
- `Moq` - Mocking library
- `coverlet.collector` - Coverage collection

### Python Dependencies

- `pytest` - Test framework
- `pytest-cov` - Coverage plugin

### JavaScript/TypeScript Dependencies

- `@testing-library/react` - React testing utilities (frontend only)
- `jsdom` - DOM environment for tests
- `vitest` - Fast test framework

## Continuous Improvement

- Review coverage reports regularly
- Add tests for bug fixes
- Refactor tests as code evolves
- Update this guide as practices change

## Troubleshooting

### Tests fail locally but pass in CI

- Check Python/Node versions match
- Ensure dependencies are synced (`uv sync`, `npm ci`)
- Check for environment-specific issues

### Import errors in Python tests

- Verify `sys.path` setup in test files
- Check module structure and `__init__.py` files
- Ensure dependencies are installed

### Vitest configuration issues

- Clear cache: `npx vitest --clearCache`
- Check `client/vitest.config.ts` and `client/src/test/setup.ts` (and `workers/vitest.config.ts`)
- Verify Vite/Vitest compatibility with dependencies

## Resources

- [pytest documentation](https://docs.pytest.org/)
- [Vitest documentation](https://vitest.dev/)
- [Testing Library](https://testing-library.com/)
- [.NET testing](https://learn.microsoft.com/en-us/dotnet/core/testing/)
